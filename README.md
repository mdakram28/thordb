<div align="center">

# ‚ö° ThorDB

**A blazingly fast, embeddable key-value storage engine written in Rust**

[![Build Status](https://img.shields.io/github/actions/workflow/status/akram/thordb/ci.yml?branch=main&style=flat-square)](https://github.com/akram/thordb/actions)
[![Crates.io](https://img.shields.io/crates/v/thordb?style=flat-square)](https://crates.io/crates/thordb)
[![Documentation](https://img.shields.io/docsrs/thordb?style=flat-square)](https://docs.rs/thordb)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue?style=flat-square)](LICENSE)
[![Rust Version](https://img.shields.io/badge/rust-1.75%2B-orange?style=flat-square)](https://www.rust-lang.org)

[Features](#features) ‚Ä¢ [Quick Start](#quick-start) ‚Ä¢ [Architecture](#architecture) ‚Ä¢ [Benchmarks](#benchmarks) ‚Ä¢ [Contributing](#contributing)

</div>

---

## Why ThorDB?

ThorDB is a **production-grade LSM-tree storage engine** designed for applications that need:

- üöÄ **High write throughput** ‚Äî LSM-tree architecture optimized for write-heavy workloads
- üîç **Fast reads** ‚Äî Binary search lookups with bloom filters (coming soon)
- üíæ **Durability** ‚Äî Write-ahead logging ensures no data loss on crashes
- üîÑ **Duplicate key support** ‚Äî First-class support for multi-version concurrency
- ü¶Ä **Pure Rust** ‚Äî Zero unsafe code, memory-safe by design
- üì¶ **Embeddable** ‚Äî Use as a library in your Rust applications

## Features

| Feature | Status |
|---------|--------|
| LSM-tree storage engine | ‚úÖ |
| Write-ahead log (WAL) | ‚úÖ |
| SSTable with binary search | ‚úÖ |
| Buffer pool with clock eviction | ‚úÖ |
| Duplicate key support | ‚úÖ |
| Crash recovery | ‚úÖ |
| Range scans | ‚úÖ |
| Tombstone garbage collection | üöß |
| Bloom filters | üöß |
| Compaction | üöß |
| Compression (LZ4/Zstd) | üìã |
| Transactions | üìã |

‚úÖ Complete | üöß In Progress | üìã Planned

## Quick Start

Add ThorDB to your `Cargo.toml`:

```toml
[dependencies]
thordb = "0.1"
```

### Basic Usage

```rust
use thordb::lsm::{LsmTree, LsmConfig, Key, Value};
use std::path::PathBuf;

fn main() -> std::io::Result<()> {
    // Open or create a database
    let config = LsmConfig {
        data_dir: PathBuf::from("./my_database"),
        memtable_size_threshold: 4 * 1024 * 1024, // 4MB
    };
    let db = LsmTree::open(config)?;

    // Write data
    db.put(Key::from("user:1"), Value::from(r#"{"name": "Alice"}"#))?;
    db.put(Key::from("user:2"), Value::from(r#"{"name": "Bob"}"#))?;

    // Read data
    if let Some(value) = db.get(&Key::from("user:1"))? {
        println!("Found: {}", String::from_utf8_lossy(value.as_bytes()));
    }

    // Delete data
    db.delete(Key::from("user:2"))?;

    // Range scan
    for entry in db.scan_live()? {
        println!("{:?} -> {:?}", entry.key, entry.value);
    }

    // Flush to disk
    db.flush()?;

    Ok(())
}
```

### Duplicate Keys (Multi-Version)

ThorDB natively supports multiple values per key with sequence numbers:

```rust
// Write multiple versions
db.put(Key::from("config"), Value::from("v1"))?;
db.put(Key::from("config"), Value::from("v2"))?;
db.put(Key::from("config"), Value::from("v3"))?;

// Get latest version
let latest = db.get(&Key::from("config"))?; // Returns "v3"

// Get all versions (newest first)
let all_versions = db.get_all(&Key::from("config"))?;
for entry in all_versions {
    println!("seq={}: {:?}", entry.seq_num, entry.value);
}
```

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ThorDB                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   Write     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MemTable   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   SSTable   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ   Request   ‚îÇ    ‚îÇ  (BTreeMap) ‚îÇ    ‚îÇ  (On-Disk)  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ         ‚îÇ                                    ‚ñ≤              ‚îÇ
‚îÇ         ‚ñº                                    ‚îÇ              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ     WAL     ‚îÇ                     ‚îÇ  Buffer Pool  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  (Durability)‚îÇ                    ‚îÇ (Page Cache)  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

| Component | Description |
|-----------|-------------|
| **MemTable** | In-memory sorted map (BTreeMap) for fast writes |
| **WAL** | Write-ahead log for durability before memtable insertion |
| **SSTable** | Immutable sorted files with binary search lookup |
| **Buffer Pool** | LRU/Clock page cache for efficient disk I/O |
| **Merge Iterator** | Efficiently combines data from multiple sources |

### Write Path

1. Log operation to WAL (durability)
2. Insert into MemTable (in-memory)
3. When MemTable is full, flush to SSTable
4. Background compaction merges SSTables

### Read Path

1. Check MemTable first (newest data)
2. Check SSTables from newest to oldest
3. Binary search within each SSTable
4. Merge results for duplicate keys

## Benchmarks

Benchmarks run on Apple M-series, comparing ThorDB against RocksDB, Sled, and LevelDB.

### Sequential Writes (1,000 keys, 100B values)

| Database | Time | Throughput |
|----------|------|------------|
| **ThorDB** | 3.06 ms | 327 ops/sec |
| RocksDB | 3.81 ms | 262 ops/sec |
| Sled | 12.5 ms | 80 ops/sec |
| LevelDB | 2.08 ms | 480 ops/sec |

### Sequential Writes (10,000 keys, 100B values)

| Database | Time | Throughput |
|----------|------|------------|
| **ThorDB** | 28.7 ms | 349 ops/sec |
| RocksDB | 33.4 ms | 300 ops/sec |
| Sled | 42.8 ms | 234 ops/sec |
| LevelDB | 19.5 ms | 512 ops/sec |

### Random Reads (from 10,000 keys)

| Database | Latency | Throughput |
|----------|---------|------------|
| LevelDB | 0.83 ¬µs | 1.2M ops/sec |
| Sled | 0.95 ¬µs | 1.0M ops/sec |
| RocksDB | 1.24 ¬µs | 800K ops/sec |
| **ThorDB** | 148 ¬µs | 6.8K ops/sec |

### Mixed Workload (80% reads, 20% writes)

| Database | Time | Throughput |
|----------|------|------------|
| **ThorDB** | 1.24 ms | 806 ops/sec |
| LevelDB | 1.25 ms | 800 ops/sec |
| RocksDB | 2.03 ms | 493 ops/sec |
| Sled | 10.2 ms | 98 ops/sec |

> **Note**: ThorDB currently lacks bloom filters and has unoptimized read paths. Read performance improvements are on the roadmap.

Run benchmarks yourself:
```bash
cargo bench --bench comparison
```

## Project Structure

```
thordb/
‚îú‚îÄ‚îÄ core/                    # Core storage engine
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ lsm/             # LSM-tree implementation
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ memtable.rs  # In-memory sorted table
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ sstable.rs   # Sorted string tables
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ wal.rs       # Write-ahead log
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ iterator.rs  # Merge iterators
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ lsm.rs       # Main coordinator
‚îÇ       ‚îú‚îÄ‚îÄ bufferpool.rs    # Page buffer pool
‚îÇ       ‚îú‚îÄ‚îÄ page.rs          # Page abstraction
‚îÇ       ‚îî‚îÄ‚îÄ tuple/           # Tuple serialization
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ main.rs              # CLI (coming soon)
```

## Contributing

We welcome contributions! Here's how to get started:

```bash
# Clone the repository
git clone https://github.com/akram/thordb.git
cd thordb

# Run tests
cargo test

# Run with logging
RUST_LOG=debug cargo run

# Format code
cargo fmt

# Run clippy
cargo clippy
```

### Areas We Need Help

- üß™ **Testing** ‚Äî More edge cases and stress tests
- üìä **Benchmarking** ‚Äî Performance comparisons with other engines
- üìñ **Documentation** ‚Äî API docs and tutorials
- üîß **Features** ‚Äî Compaction, bloom filters, compression

## Roadmap

### v0.2 (Next) ‚Äî Read Performance
- [ ] Bloom filters for faster negative lookups
- [ ] Block cache for hot data
- [ ] Read path optimization (100x improvement target)
- [ ] Large value support (values > page size)

### v0.3 ‚Äî Compaction & Compression
- [ ] Level-based compaction
- [ ] Size-tiered compaction
- [ ] LZ4/Zstd compression

### v0.4 ‚Äî Production Features
- [ ] Snapshots and iterators
- [ ] Configurable compaction strategies
- [ ] Metrics and observability

### v1.0 ‚Äî Enterprise Ready
- [ ] Full ACID transactions
- [ ] Replication support
- [ ] Production-ready stability

## Inspiration

ThorDB draws inspiration from these excellent projects:

- [RocksDB](https://rocksdb.org/) ‚Äî The industry-standard LSM engine
- [LevelDB](https://github.com/google/leveldb) ‚Äî Google's original LSM implementation
- [Sled](https://sled.rs/) ‚Äî Modern embedded database in Rust
- [Mini-LSM](https://github.com/skyzh/mini-lsm) ‚Äî Educational LSM implementation

## License

ThorDB is dual-licensed under:

- [MIT License](LICENSE-MIT)
- [Apache License 2.0](LICENSE-APACHE)

Choose whichever license works best for your project.

---

<div align="center">

**If you find ThorDB useful, please consider giving it a ‚≠ê**

Made with ‚ù§Ô∏è and ü¶Ä

</div>
